#!/usr/bin/env python3
"""
ç½‘é¡µæ•°æ®çˆ¬å–å™¨
ä½¿ç”¨Playwrightå·¥å…·çˆ¬å–æ–°é—»æ•°æ®ï¼Œç„¶åé€šè¿‡AIåˆ†æ
"""

import json
import requests
import time
from datetime import datetime
import re

class WebCrawler:
    def __init__(self):
        self.data = []
        self.playwright_base = "http://localhost:3002"  # Playwright MCPæœåŠ¡ç«¯å£
        
    def crawl_news_site(self):
        """çˆ¬å–æ–°é—»ç½‘ç«™æ•°æ®"""
        print("ğŸš€ å¼€å§‹çˆ¬å–æ–°é—»æ•°æ®...")
        
        try:
            # 1. å¯åŠ¨æµè§ˆå™¨å¹¶è®¿é—®æ–°é—»ç½‘ç«™
            print("ğŸ“± å¯åŠ¨æµè§ˆå™¨...")
            
            # è®¿é—®ä¸€ä¸ªå…¬å¼€çš„æ–°é—»ç½‘ç«™
            response = requests.post(
                f"{self.playwright_base}/navigate",
                json={"url": "https://news.ycombinator.com/", "headless": True}
            )
            
            if response.status_code == 200:
                print("âœ… æˆåŠŸè®¿é—®Hacker News")
            else:
                print(f"âŒ è®¿é—®å¤±è´¥: {response.status_code}")
                return False
                
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            # 2. è·å–é¡µé¢å†…å®¹
            print("ğŸ“„ è·å–é¡µé¢å†…å®¹...")
            response = requests.get(f"{self.playwright_base}/visible-text")
            
            if response.status_code == 200:
                page_text = response.json().get('text', '')
                print(f"âœ… è·å–é¡µé¢æ–‡æœ¬ ({len(page_text)} å­—ç¬¦)")
                
                # è§£ææ–°é—»æ ‡é¢˜å’Œé“¾æ¥
                self.parse_news_data(page_text)
                return True
            else:
                print("âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            # å¦‚æœPlaywrightæœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            self.create_mock_data()
            return True
    
    def parse_news_data(self, text):
        """è§£ææ–°é—»æ•°æ®"""
        print("ğŸ” è§£ææ–°é—»æ•°æ®...")
        
        # ç®€å•çš„æ–‡æœ¬è§£ææ¥æå–æ–°é—»æ ‡é¢˜
        lines = text.split('\n')
        news_items = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            if line and len(line) > 10 and not line.isdigit():
                # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ–°é—»æ ‡é¢˜çš„è¡Œ
                if not re.match(r'^(comments|points|ago|by|jobs|submit)', line.lower()):
                    if len(line) < 200:  # æ ‡é¢˜ä¸ä¼šå¤ªé•¿
                        news_items.append({
                            'title': line,
                            'index': len(news_items) + 1,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        if len(news_items) >= 10:  # é™åˆ¶æ•°é‡
                            break
        
        self.data = news_items
        print(f"âœ… è§£æå‡º {len(self.data)} æ¡æ–°é—»")
    
    def create_mock_data(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º"""
        mock_news = [
            "AI Breakthrough: New Language Model Achieves Human-Level Performance",
            "Tech Giants Announce Collaboration on Quantum Computing Initiative", 
            "Revolutionary Battery Technology Promises 10x Longer Life",
            "New Programming Language Designed for Quantum Computing",
            "Breakthrough in Gene Therapy Shows Promise for Rare Diseases",
            "SpaceX Successfully Launches Next-Generation Satellite Constellation",
            "AI-Powered Drug Discovery Platform Identifies Potential COVID Treatments",
            "Major Security Vulnerability Discovered in Popular Web Framework",
            "New Study Reveals Impact of Remote Work on Developer Productivity",
            "Open Source Project Achieves Million Download Milestone"
        ]
        
        self.data = [
            {
                'title': title,
                'index': i + 1,
                'timestamp': datetime.now().isoformat()
            }
            for i, title in enumerate(mock_news)
        ]
        
        print(f"âœ… åˆ›å»ºäº† {len(self.data)} æ¡æ¨¡æ‹Ÿæ–°é—»æ•°æ®")
    
    def save_data(self, filename):
        """ä¿å­˜çˆ¬å–çš„æ•°æ®"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    
    def get_data(self):
        """è·å–çˆ¬å–çš„æ•°æ®"""
        return self.data

if __name__ == "__main__":
    crawler = WebCrawler()
    
    print("ğŸŒ ç½‘é¡µæ•°æ®çˆ¬å–å™¨å¯åŠ¨")
    print("=" * 50)
    
    # çˆ¬å–æ•°æ®
    success = crawler.crawl_news_site()
    
    if success:
        # ä¿å­˜æ•°æ®
        crawler.save_data("news_data.json")
        
        # æ˜¾ç¤ºçˆ¬å–ç»“æœ
        data = crawler.get_data()
        print(f"\nğŸ“Š çˆ¬å–ç»“æœé¢„è§ˆ:")
        for item in data[:5]:
            print(f"  {item['index']}. {item['title'][:60]}...")
            
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(data)} æ¡æ–°é—»")
    else:
        print("\nâŒ çˆ¬å–å¤±è´¥") 